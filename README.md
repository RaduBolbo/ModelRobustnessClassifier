# ModelRobustnessClassifier

In this project we explore the robustness of a deep learning classification model trained on [Animals-10](https://www.kaggle.com/datasets/alessiocorrado99/animals10) dataset. The dataset contains images with 10 animal classes (dog, cat, horse, spyder, butterfly, chicken, sheep, cow, squirrel, elephant). We implement 2 attack mechanisms (PGD, DDN) and 2 defense strategies (Feature Squeezing and Defensive Distillation).

## Attacks

1. 
2. 

## Defenses

### 1. **Feature squeezing**

Feature squeezing is a defense mechanism that reduces the complexity of input data to limit the space available for adversarial perturbations. By simplifying or "squeezing" the features of an input, it becomes harder for adversarial attacks to manipulate the model's output.

Common techniques include:

1. **Bit-Depth Reduction**: Reducing the color depth of images by limiting the number of bits per pixel.
2. **Median Filtering**: Applying a filter to smooth the image and remove high-frequency noise that may contain adversarial perturbations.
3. **Resampling**: Downscaling and rescaling the image to the original size. It removes subte perturbations introduces by adversarial attacks while mentaining the image structure.

The defense works as follows: 

1. Apply filters on the input image to obtain squeezed versions.
2. Use the classification model on the original input and the squeezed versions. The model output logits must be passed through a softmax layer.
3. Compute L1-norm between the original softmax output and squeezed softmax outputs. Given that the vectors are softmax probabilities, L1 will take values between 0 (represents a legitimate image) and 2 (represents an adversarial input). If there are multiple squeezed inputs, the chosen L1 distance is the one with the maximum value.
4. Set a threshold (we chose 1), compare the maximum distance with the threshold and decide if the input is adversarial or not.

### 2. Defensive Distillation

Defensive distillations is inspired from knowledge distillation, where a "student" network is trained to mimic the behavior of a "teacher" network. The key idea is to train the teacher network with standard methods and then use it to generate soft labels (probability distributions over classes) for the training data. These soft labels are produced at a high temperature in the softmax function to emphasize similarities between classes. The student network is then trained on these softened probabilities rather than hard labels. This process reduces the sensitivity of the student network to small perturbations in the input, effectively smoothing the decision boundaries and making it harder for adversarial attacks to exploit them. 

Defensive distillation steps:

1. Train teacher network using a softmax function with temperature T>1. The higher temperature shows the similarities between classes, providing richer information about the data structure.

2. Train the student network using the soft labels generated by the teacher network. In the student network the softmax temperature is set to the same T as the one from the teacher network.

3. During inference or evaluation, only the student network is used with softmax temperature T=1. The student network is expected to have smoother decision boundaries, making it more robust to adversarial perturbations.

## Linux setup

To setup the project run the following commands:

```bash
git clone https://github.com/RaduBolbo/ModelRobustnessClassifier.git
cd ModelRobustnessClassifier/
```

The script setup.sh does the following:
- Installs python3, pip, kaggle and python venv if it does not exist
- Downloads the dataset from kaggle (one needs to set the username and API key for this to work. Go to kaggle at Settings -> API -> Create New Token. This will download a json file where you can find the username and key. Then go to setup.sh and set the Kaggle credentials)
- Creates checkpoints directory -> model checkpoints can be found here (to be added)
- Creates python virtual environment and installs the packages from *requirements.txt*

To run the script:
```bash
bash setup.sh
```

Activate the virtual environment:
```bash
source myenv/bin/activate
```

To train the baseline model:
```bash
python src/train_loop_baseline.py 
```