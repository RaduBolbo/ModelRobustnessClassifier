# ModelRobustnessClassifier

In this project we explore the robustness of a deep learning classification model trained on [Animals-10](https://www.kaggle.com/datasets/alessiocorrado99/animals10) dataset. The dataset contains images with 10 animal classes (dog, cat, horse, spyder, butterfly, chicken, sheep, cow, squirrel, elephant). We implement 2 attack mechanisms (PGD, DDN) and 2 defense strategies (Feature Squeezing and Defensive Distillation).

## Attacks

1. **Projected Gradient Descent (PGD)**  
   PGD is a powerful adversarial attack that iteratively updates the input by taking steps in the direction of the gradient of the loss function, ensuring each perturbation stays within a specified norm constraint (e.g., \(L_\infty\) or \(L_2\)). It is often used to evaluate the robustness of machine learning models.

2. **Decoupled Direction and Norm (DDN)**  
   DDN is designed to minimize the norm of adversarial perturbations. It works by decoupling the direction and norm adjustments during the attack process, iteratively reducing the perturbation magnitude while maintaining adversarial success. This makes DDN particularly effective for generating imperceptible adversarial examples.


## Defenses

### 1. **Feature squeezing**

Feature squeezing is a defense mechanism that reduces the complexity of input data to limit the space available for adversarial perturbations. By simplifying or "squeezing" the features of an input, it becomes harder for adversarial attacks to manipulate the model's output.

Common techniques include:

1. **Bit-Depth Reduction**: Reducing the color depth of images by limiting the number of bits per pixel.
2. **Median Filtering**: Applying a filter to smooth the image and remove high-frequency noise that may contain adversarial perturbations.
3. **Resampling**: Downscaling and rescaling the image to the original size. It removes subte perturbations introduces by adversarial attacks while mentaining the image structure.

The defense works as follows: 

1. Apply filters on the input image to obtain squeezed versions.
2. Use the classification model on the original input and the squeezed versions. The model output logits must be passed through a softmax layer.
3. Compute L1-norm between the original softmax output and squeezed softmax outputs. Given that the vectors are softmax probabilities, L1 will take values between 0 (represents a legitimate image) and 2 (represents an adversarial input). If there are multiple squeezed inputs, the chosen L1 distance is the one with the maximum value.
4. Set a threshold (we chose 1), compare the maximum distance with the threshold and decide if the input is adversarial or not.

### 2. Defensive Distillation

Defensive distillations is inspired from knowledge distillation, where a "student" network is trained to mimic the behavior of a "teacher" network. The key idea is to train the teacher network with standard methods and then use it to generate soft labels (probability distributions over classes) for the training data. These soft labels are produced at a high temperature in the softmax function to emphasize similarities between classes. The student network is then trained on these softened probabilities rather than hard labels. This process reduces the sensitivity of the student network to small perturbations in the input, effectively smoothing the decision boundaries and making it harder for adversarial attacks to exploit them. 

Defensive distillation steps:

1. Train teacher network using a softmax function with temperature T>1. The higher temperature shows the similarities between classes, providing richer information about the data structure.

2. Train the student network using the soft labels generated by the teacher network. In the student network the softmax temperature is set to the same T as the one from the teacher network.

3. During inference or evaluation, only the student network is used with softmax temperature T=1. The student network is expected to have smoother decision boundaries, making it more robust to adversarial perturbations.

## Linux setup

Download the dataset and checkpoints from [this](https://ctipub-my.sharepoint.com/:f:/g/personal/cerasela_manolache_stud_etti_upb_ro/Eh1QFsABCsBBhcEd-huGm7YBO-Ts773jnCrabxXKWKVsWA?e=Q3zib9) link. Make sure to unzip raw-img.zip from the dataset directory.

To setup the project run the following commands:

```bash
git clone https://github.com/RaduBolbo/ModelRobustnessClassifier.git
cd ModelRobustnessClassifier/
```

The script setup.sh does the following:
- Installs python3, pip and python venv if it does not exist
- Creates python virtual environment and installs the packages from *requirements.txt*

To run the script:
```bash
bash setup.sh
```

Activate the virtual environment:
```bash
source myenv/bin/activate
```

## Using the repo

The main script for testing the baseline model with attacks and defenses is attack_defense.py. It contains a class that allows choosing an attack (PGD or DNN) and a defense (Feature Squezzing or Defensive Distillation) and the attack/defense parameters. The script provides some examples on how to use the class. 

To run:
```bash
python src/attack_defense.py 
```


To train the baseline model:
```bash
python src/train_loop_baseline.py 
```

To test a model on the validation set:
```bash
python src/evaluate_model.py 
```
